{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/wemblee/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/wemblee/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/wemblee/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/wemblee/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/wemblee/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/wemblee/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/wemblee/anaconda3/lib/python3.7/site-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#from gensim.models import Word2Vec\n",
    "from similarities import execute_column_evaluations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.metrics import CosineSimilarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from build_dataset import get_datasets_built\n",
    "import time\n",
    "import get_labels\n",
    "import importlib\n",
    "from execute_coclustring import execute_clustering_evaluation\n",
    "from nlp_preprocessing import nlp_prep, processCorpus\n",
    "#importlib.reload(get_labels)\n",
    "#%reload_ext autoreload\n",
    "#%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#Test pipelinefrom similarities import execute_column_evaluationsfrom similarities import execute_column_evaluationsfrom similarities import execute_column_evaluations\n",
    "data_r8, classes_count, stemmed = get_datasets_built('r8')\n",
    "###-----------True labels list\n",
    "true_labels = get_labels.get_true_labels(data_r8['class'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7673\r"
     ]
    }
   ],
   "source": [
    "###-----------Executing the preprocessing\n",
    "data_r8_processed = data_r8\n",
    "if(not stemmed):\n",
    "    for index ,row in data_r8_processed.iterrows():\n",
    "        print(index, end=\"\\r\")\n",
    "        row['text'] = nlp_prep(row['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Test preprocessing\n",
    "print(\"first preprocessing: \", len(nlp_prep(data_r8.iloc[5][\"text\"])))\n",
    "print(\"original: \", len(data_r8.iloc[5][\"text\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DATASET---- r8\n",
      "preprocessing finished\n",
      "---executing  CoclustMod\n",
      "NMI : 0.35110296497684373\n",
      "ARI : 0.2976896678074259\n",
      "Accuracy : 0.4693771175397446\n",
      "---executing  CoclustInfo\n",
      "NMI : 0.41542132095898676\n",
      "ARI : 0.23876042749201964\n",
      "Accuracy : 0.3616106333072713\n",
      "---executing  CoclustModFuzzy\n",
      "NMI : 0.47719071468224405\n",
      "ARI : 0.34927249797899385\n",
      "Accuracy : 0.6215793588741204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wemblee/anaconda3/lib/python3.7/site-packages/coclust/coclustering/coclust_mod.py:97: FutureWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
      "  warn_on_dtype=False, estimator=None)\n",
      "/home/wemblee/anaconda3/lib/python3.7/site-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n",
      "/home/wemblee/anaconda3/lib/python3.7/site-packages/coclust/coclustering/coclust_info.py:97: FutureWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
      "  warn_on_dtype=False, estimator=None)\n",
      "/home/wemblee/anaconda3/lib/python3.7/site-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n",
      "/home/wemblee/anaconda3/lib/python3.7/site-packages/coclust/coclustering/coclust_spec_mod.py:87: FutureWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
      "  warn_on_dtype=False, estimator=None)\n",
      "/home/wemblee/anaconda3/lib/python3.7/site-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "##------------CO-CLUSTERING START FROM HERE-------------##\n",
    "datasets = ['r8']#, \"classic3\", \"classic4\", \"r40\", \"r52\", \"webkb\"]#\"ng5\", \"ng20\"\n",
    "evaluation_matrix = np.empty((0, 5))\n",
    "evaluation_list = []\n",
    "columns_labels_coclustering = []\n",
    "#vocabularry = []\n",
    "for data_set in datasets:\n",
    "    print(\"---DATASET----\", data_set)\n",
    "    data, classes_count, stemmed = get_datasets_built(data_set)\n",
    "    ###-----------True labels list\n",
    "    true_labels = get_labels.get_true_labels(data['class'])\n",
    "    ###-----------Executing the preprocessing\n",
    "    data_processed = data\n",
    "    if(not stemmed):\n",
    "        for index ,row in data_processed.iterrows():\n",
    "            print(index, end=\"\\r\")\n",
    "            row['text'] = nlp_prep(row['text'])\n",
    "    print(\"preprocessing finished\")\n",
    "    ###----------Executing the coclustering\n",
    "    evaluation_list, coclusetring_cols = execute_clustering_evaluation(data_processed['text'], true_labels, use_words_thresh=True, max_iteration=100)\n",
    "    columns_labels_coclustering += [[coclusetring_cols]]\n",
    "    evaluation_array = np.array(evaluation_list)\n",
    "    evaluation_datasetname = np.array([[data_set]] * len(evaluation_array))\n",
    "    evaluation_array = np.concatenate( (evaluation_array, evaluation_datasetname), axis=1)\n",
    "    evaluation_matrix = np.append(evaluation_matrix, evaluation_array, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "evaluation_df = pd.DataFrame(evaluation_matrix, columns=[ \"method\", \"nmi\", \"ari\", \"acc\", \"dataset\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "evaluation_df.to_csv(\"results/all_coclustring_results.csv\", sep=',', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "columns_labels_coclustering_df = pd.DataFrame(columns_labels_coclustering)\n",
    "columns_labels_coclustering_df.to_csv(\"results/columns_labels_coclustering_df.csv\", sep=',', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[[[[[1,\n     1,\n     1,\n     2,\n     1,\n     1,\n     1,\n     2,\n     2,\n     1,\n     1,\n     0,\n     2,\n     1,\n     0,\n     1,\n     1,\n     0,\n     1,\n     7,\n     7,\n     1,\n     6,\n     1,\n     1,\n     0,\n     1,\n     0,\n     0,\n     3,\n     1,\n     1,\n     1,\n     0,\n     2,\n     1,\n     1,\n     1,\n     2,\n     7,\n     1,\n     1,\n     2,\n     6,\n     1,\n     1,\n     2,\n     2,\n     2,\n     2,\n     6,\n     1,\n     2,\n     1,\n     1,\n     6,\n     1,\n     1,\n     0,\n     2,\n     1,\n     1,\n     4,\n     0,\n     1,\n     0,\n     0,\n     1,\n     1,\n     1,\n     4,\n     5,\n     1,\n     6,\n     6,\n     3,\n     6,\n     3,\n     5,\n     2,\n     1,\n     1,\n     5,\n     6,\n     0,\n     0,\n     7,\n     1,\n     1,\n     1,\n     1,\n     0,\n     2,\n     2,\n     1,\n     2,\n     1,\n     1,\n     3,\n     1,\n     6,\n     3,\n     2,\n     7,\n     4,\n     1,\n     3,\n     1,\n     7,\n     0,\n     1,\n     2,\n     2,\n     5,\n     1,\n     0,\n     7,\n     0,\n     0,\n     1,\n     4,\n     7,\n     0,\n     5,\n     1,\n     2,\n     1,\n     1,\n     7,\n     6,\n     2,\n     1,\n     2,\n     1,\n     4,\n     2,\n     1,\n     0,\n     0,\n     5,\n     2,\n     1,\n     1,\n     1,\n     0,\n     0,\n     5,\n     1,\n     1,\n     1,\n     1,\n     1,\n     6,\n     2,\n     0,\n     4,\n     1,\n     1,\n     1,\n     1,\n     0,\n     1,\n     0,\n     1,\n     3,\n     1,\n     1,\n     1,\n     2,\n     1,\n     1,\n     2,\n     2,\n     0,\n     1,\n     7,\n     1,\n     2,\n     0,\n     0,\n     0,\n     4,\n     4,\n     0,\n     3,\n     1,\n     2,\n     1,\n     1,\n     1,\n     2,\n     1,\n     7,\n     1,\n     5,\n     3,\n     3,\n     7,\n     1,\n     1,\n     1,\n     2,\n     2,\n     0,\n     0,\n     0,\n     1,\n     0,\n     1,\n     7,\n     1,\n     1,\n     5,\n     1,\n     0,\n     1,\n     2,\n     1,\n     7,\n     1,\n     7,\n     1,\n     1,\n     1,\n     3,\n     1,\n     0,\n     1,\n     1,\n     5,\n     0,\n     2,\n     1,\n     0,\n     1,\n     4,\n     0,\n     2,\n     4,\n     1,\n     4,\n     3,\n     1,\n     0,\n     5,\n     7,\n     0,\n     4,\n     0,\n     2,\n     0,\n     3,\n     0,\n     1,\n     0,\n     0,\n     7,\n     2,\n     1,\n     1,\n     6,\n     1,\n     1,\n     1,\n     1,\n     1,\n     2,\n     1,\n     3,\n     5,\n     7,\n     5,\n     6,\n     2,\n     2,\n     1,\n     0,\n     1,\n     2,\n     1,\n     1,\n     1,\n     1,\n     7,\n     6,\n     2,\n     0,\n     1,\n     1,\n     1,\n     2,\n     1,\n     0,\n     5,\n     1,\n     1,\n     0,\n     1,\n     1,\n     1,\n     2,\n     0,\n     1,\n     1,\n     1,\n     3,\n     1,\n     6,\n     0,\n     1,\n     6,\n     1,\n     1,\n     2,\n     0,\n     0,\n     1,\n     1,\n     2,\n     1,\n     2,\n     2,\n     1,\n     1,\n     1,\n     1,\n     0,\n     1,\n     6,\n     3,\n     6,\n     1,\n     1,\n     1,\n     4,\n     4,\n     4,\n     2,\n     2,\n     3,\n     6,\n     7,\n     6,\n     4,\n     1,\n     5,\n     7,\n     1,\n     4,\n     6,\n     1,\n     1,\n     3,\n     1,\n     1,\n     1,\n     1,\n     1,\n     2,\n     3,\n     2,\n     0,\n     6,\n     1,\n     4,\n     0,\n     2,\n     1,\n     0,\n     2,\n     7,\n     7,\n     0,\n     0,\n     0,\n     1,\n     1,\n     1,\n     1,\n     6,\n     3,\n     3,\n     1,\n     1,\n     2,\n     3,\n     1,\n     1,\n     2,\n     2,\n     3,\n     2,\n     0,\n     4,\n     1,\n     1,\n     4,\n     0,\n     5,\n     1,\n     1,\n     7,\n     2,\n     1,\n     5,\n     1,\n     0,\n     4,\n     1,\n     6,\n     1,\n     1,\n     1,\n     1,\n     1,\n     1,\n     3,\n     1,\n     6,\n     0,\n     7,\n     0,\n     1,\n     1,\n     4,\n     1,\n     0,\n     1,\n     2,\n     2,\n     2,\n     0,\n     1,\n     2,\n     1,\n     1,\n     7,\n     1,\n     7,\n     3,\n     7,\n     0,\n     3,\n     1,\n     1,\n     1,\n     3,\n     2,\n     0,\n     2,\n     4,\n     6,\n     0,\n     1,\n     1,\n     1,\n     1,\n     2,\n     1,\n     1,\n     2,\n     2,\n     6,\n     4,\n     4,\n     2,\n     1,\n     1,\n     0,\n     1,\n     5,\n     5,\n     1,\n     1,\n     6,\n     5,\n     2,\n     1,\n     0,\n     1,\n     6,\n     1,\n     2,\n     6,\n     1,\n     1,\n     0,\n     1,\n     1,\n     1,\n     0,\n     0,\n     0,\n     3,\n     7,\n     2,\n     1,\n     1,\n     5,\n     2,\n     2,\n     1,\n     1,\n     1,\n     0,\n     6,\n     3,\n     0,\n     0,\n     0,\n     0,\n     0,\n     1,\n     3,\n     2,\n     1,\n     1,\n     0,\n     0,\n     6,\n     1,\n     1,\n     1,\n     1,\n     6,\n     5,\n     6,\n     1,\n     1,\n     0,\n     0,\n     7,\n     7,\n     0,\n     1,\n     0,\n     1,\n     1,\n     1,\n     2,\n     0,\n     0,\n     2,\n     1,\n     1,\n     2,\n     0,\n     0,\n     4,\n     2,\n     1,\n     0,\n     0,\n     7,\n     1,\n     1,\n     1,\n     2,\n     2,\n     1,\n     0,\n     2,\n     1,\n     1,\n     1,\n     2,\n     1,\n     1,\n     1,\n     6,\n     1,\n     1,\n     1,\n     6,\n     0,\n     6,\n     6,\n     3,\n     2,\n     5,\n     1,\n     6,\n     1,\n     1,\n     2,\n     2,\n     0,\n     7,\n     1,\n     1,\n     0,\n     0,\n     0,\n     2,\n     1,\n     1,\n     1,\n     1,\n     1,\n     2,\n     3,\n     0,\n     1,\n     3,\n     0,\n     6,\n     1,\n     1,\n     6,\n     1,\n     0,\n     1,\n     0,\n     2,\n     1,\n     0,\n     1,\n     1,\n     1,\n     1,\n     6,\n     1,\n     0,\n     0,\n     7,\n     0,\n     6,\n     1,\n     6,\n     7,\n     1,\n     1,\n     5,\n     6,\n     1,\n     0,\n     1,\n     1,\n     3,\n     2,\n     1,\n     2,\n     1,\n     4,\n     7,\n     1,\n     1,\n     6,\n     1,\n     0,\n     1,\n     7,\n     1,\n     1,\n     1,\n     1,\n     1,\n     1,\n     6,\n     6,\n     6,\n     1,\n     3,\n     2,\n     4,\n     7,\n     1,\n     1,\n     7,\n     1,\n     4,\n     1,\n     5,\n     5,\n     1,\n     3,\n     0,\n     3,\n     1,\n     1,\n     3,\n     2,\n     7,\n     5,\n     5,\n     2,\n     0,\n     1,\n     1,\n     2,\n     0,\n     1,\n     7,\n     1,\n     2,\n     7,\n     1,\n     1,\n     1,\n     7,\n     1,\n     1,\n     5,\n     1,\n     1,\n     1,\n     7,\n     1,\n     4,\n     1,\n     1,\n     0,\n     7,\n     1,\n     1,\n     0,\n     1,\n     3,\n     2,\n     1,\n     1,\n     1,\n     1,\n     1,\n     1,\n     4,\n     1,\n     1,\n     1,\n     1,\n     1,\n     7,\n     1,\n     1,\n     6,\n     1,\n     1,\n     1,\n     1,\n     7,\n     7,\n     1,\n     3,\n     1,\n     1,\n     6,\n     2,\n     0,\n     2,\n     1,\n     0,\n     1,\n     2,\n     2,\n     1,\n     1,\n     0,\n     2,\n     3,\n     0,\n     7,\n     6,\n     3,\n     0,\n     1,\n     1,\n     1,\n     1,\n     0,\n     0,\n     7,\n     1,\n     1,\n     1,\n     1,\n     2,\n     1,\n     1,\n     2,\n     1,\n     6,\n     1,\n     0,\n     2,\n     7,\n     3,\n     1,\n     3,\n     1,\n     0,\n     1,\n     5,\n     2,\n     0,\n     7,\n     7,\n     1,\n     1,\n     1,\n     0,\n     1,\n     5,\n     0,\n     2,\n     1,\n     1,\n     0,\n     0,\n     0,\n     1,\n     5,\n     0,\n     5,\n     7,\n     2,\n     1,\n     4,\n     1,\n     1,\n     2,\n     6,\n     0,\n     1,\n     0,\n     1,\n     1,\n     7,\n     1,\n     2,\n     2,\n     1,\n     2,\n     1,\n     1,\n     1,\n     1,\n     1,\n     2,\n     1,\n     0,\n     0,\n     1,\n     0,\n     4,\n     1,\n     1,\n     2,\n     2,\n     7,\n     0,\n     3,\n     1,\n     1,\n     7,\n     3,\n     1,\n     1,\n     1,\n     1,\n     0,\n     1,\n     1,\n     1,\n     1,\n     3,\n     3,\n     6,\n     2,\n     5,\n     2,\n     3,\n     1,\n     7,\n     5,\n     3,\n     1,\n     1,\n     1,\n     5,\n     1,\n     2,\n     1,\n     4,\n     6,\n     0,\n     7,\n     0,\n     1,\n     3,\n     7,\n     1,\n     1,\n     2,\n     1,\n     1,\n     4,\n     2,\n     0,\n     1,\n     1,\n     1,\n     1,\n     3,\n     1,\n     0,\n     0,\n     2,\n     1,\n     2,\n     1,\n     2,\n     1,\n     1,\n     1,\n     1,\n     0,\n     5,\n     6,\n     6,\n     2,\n     2,\n     1,\n     1,\n     0,\n     1,\n     7,\n     0,\n     4,\n     6,\n     6,\n     1,\n     1,\n     0,\n     1,\n     1,\n     2,\n     2,\n     7,\n     2,\n     1,\n     1,\n     2,\n     1,\n     2,\n     1,\n     1,\n     2,\n     1,\n     1,\n     1,\n     1,\n     1,\n     1,\n     5,\n     1,\n     1,\n     6,\n     2,\n     2,\n     7,\n     0,\n     0,\n     1,\n     2,\n     2,\n     1,\n     0,\n     0,\n     1,\n     7,\n     1,\n     1,\n     1,\n     2,\n     1,\n     1,\n     3,\n     2,\n     1,\n     3,\n     1,\n     1,\n     4,\n     2,\n     2,\n     1,\n     1,\n     7,\n     5,\n     7,\n     1,\n     2,\n     5,\n     1,\n     0,\n     0,\n     1,\n     2,\n     7,\n     1,\n     ...]],\n   [[6,\n     2,\n     2,\n     7,\n     6,\n     6,\n     2,\n     6,\n     6,\n     6,\n     6,\n     6,\n     3,\n     6,\n     5,\n     6,\n     6,\n     7,\n     2,\n     1,\n     1,\n     1,\n     3,\n     2,\n     6,\n     6,\n     2,\n     7,\n     7,\n     6,\n     6,\n     6,\n     6,\n     7,\n     7,\n     2,\n     6,\n     2,\n     7,\n     4,\n     2,\n     2,\n     7,\n     5,\n     6,\n     6,\n     3,\n     3,\n     3,\n     3,\n     3,\n     2,\n     7,\n     2,\n     6,\n     7,\n     6,\n     6,\n     6,\n     6,\n     6,\n     2,\n     6,\n     7,\n     6,\n     7,\n     7,\n     2,\n     6,\n     2,\n     7,\n     3,\n     6,\n     1,\n     7,\n     6,\n     3,\n     7,\n     0,\n     1,\n     2,\n     6,\n     0,\n     3,\n     6,\n     7,\n     5,\n     6,\n     6,\n     6,\n     6,\n     6,\n     6,\n     7,\n     2,\n     3,\n     6,\n     6,\n     2,\n     2,\n     7,\n     2,\n     3,\n     5,\n     5,\n     2,\n     6,\n     6,\n     1,\n     5,\n     2,\n     6,\n     3,\n     5,\n     6,\n     7,\n     5,\n     7,\n     7,\n     2,\n     7,\n     5,\n     7,\n     7,\n     6,\n     6,\n     6,\n     6,\n     7,\n     3,\n     7,\n     2,\n     7,\n     6,\n     5,\n     3,\n     2,\n     5,\n     7,\n     3,\n     7,\n     6,\n     6,\n     2,\n     7,\n     7,\n     0,\n     2,\n     2,\n     2,\n     2,\n     6,\n     3,\n     3,\n     1,\n     3,\n     6,\n     2,\n     2,\n     6,\n     3,\n     6,\n     7,\n     2,\n     7,\n     6,\n     6,\n     2,\n     3,\n     6,\n     6,\n     6,\n     6,\n     7,\n     6,\n     5,\n     2,\n     6,\n     5,\n     7,\n     7,\n     7,\n     5,\n     7,\n     6,\n     2,\n     6,\n     2,\n     2,\n     2,\n     3,\n     6,\n     1,\n     6,\n     0,\n     7,\n     7,\n     5,\n     6,\n     6,\n     6,\n     7,\n     6,\n     7,\n     7,\n     7,\n     6,\n     5,\n     2,\n     5,\n     6,\n     2,\n     3,\n     2,\n     6,\n     6,\n     6,\n     2,\n     6,\n     6,\n     4,\n     2,\n     2,\n     6,\n     2,\n     2,\n     5,\n     2,\n     6,\n     5,\n     5,\n     6,\n     6,\n     3,\n     6,\n     7,\n     7,\n     6,\n     3,\n     6,\n     3,\n     6,\n     2,\n     7,\n     7,\n     1,\n     6,\n     3,\n     7,\n     6,\n     7,\n     5,\n     5,\n     6,\n     7,\n     7,\n     6,\n     5,\n     6,\n     6,\n     3,\n     6,\n     6,\n     6,\n     6,\n     6,\n     3,\n     6,\n     6,\n     7,\n     1,\n     0,\n     3,\n     3,\n     5,\n     6,\n     7,\n     6,\n     3,\n     2,\n     6,\n     2,\n     6,\n     1,\n     3,\n     7,\n     5,\n     2,\n     6,\n     2,\n     6,\n     2,\n     7,\n     3,\n     6,\n     2,\n     7,\n     6,\n     6,\n     2,\n     1,\n     7,\n     2,\n     6,\n     2,\n     3,\n     6,\n     3,\n     7,\n     6,\n     3,\n     6,\n     6,\n     3,\n     7,\n     7,\n     6,\n     6,\n     3,\n     6,\n     3,\n     6,\n     6,\n     6,\n     6,\n     6,\n     7,\n     6,\n     3,\n     6,\n     5,\n     2,\n     6,\n     5,\n     3,\n     6,\n     7,\n     6,\n     6,\n     3,\n     3,\n     5,\n     3,\n     3,\n     6,\n     5,\n     1,\n     2,\n     7,\n     7,\n     6,\n     6,\n     5,\n     2,\n     6,\n     6,\n     2,\n     2,\n     3,\n     5,\n     5,\n     7,\n     3,\n     2,\n     6,\n     7,\n     3,\n     2,\n     7,\n     3,\n     1,\n     1,\n     6,\n     7,\n     3,\n     6,\n     2,\n     6,\n     6,\n     5,\n     6,\n     6,\n     6,\n     6,\n     6,\n     6,\n     6,\n     6,\n     7,\n     1,\n     2,\n     6,\n     1,\n     5,\n     6,\n     2,\n     3,\n     7,\n     5,\n     6,\n     6,\n     1,\n     7,\n     2,\n     5,\n     6,\n     7,\n     7,\n     2,\n     7,\n     2,\n     1,\n     6,\n     6,\n     6,\n     6,\n     7,\n     2,\n     5,\n     3,\n     1,\n     5,\n     6,\n     6,\n     7,\n     6,\n     7,\n     6,\n     7,\n     6,\n     3,\n     7,\n     5,\n     3,\n     2,\n     2,\n     1,\n     7,\n     1,\n     6,\n     1,\n     3,\n     5,\n     3,\n     6,\n     6,\n     7,\n     3,\n     3,\n     3,\n     6,\n     3,\n     7,\n     2,\n     2,\n     2,\n     2,\n     3,\n     6,\n     2,\n     7,\n     6,\n     0,\n     3,\n     5,\n     6,\n     7,\n     6,\n     5,\n     6,\n     0,\n     5,\n     6,\n     7,\n     3,\n     0,\n     7,\n     2,\n     7,\n     6,\n     3,\n     2,\n     5,\n     3,\n     6,\n     6,\n     7,\n     2,\n     1,\n     6,\n     5,\n     7,\n     6,\n     6,\n     5,\n     7,\n     6,\n     7,\n     5,\n     7,\n     7,\n     6,\n     2,\n     2,\n     7,\n     7,\n     2,\n     5,\n     5,\n     7,\n     7,\n     7,\n     6,\n     7,\n     0,\n     6,\n     2,\n     7,\n     7,\n     3,\n     2,\n     6,\n     2,\n     6,\n     3,\n     0,\n     5,\n     2,\n     6,\n     7,\n     7,\n     6,\n     6,\n     7,\n     6,\n     3,\n     6,\n     6,\n     2,\n     6,\n     7,\n     7,\n     7,\n     6,\n     6,\n     6,\n     7,\n     7,\n     5,\n     6,\n     6,\n     5,\n     7,\n     5,\n     6,\n     6,\n     6,\n     6,\n     3,\n     6,\n     6,\n     3,\n     6,\n     2,\n     6,\n     3,\n     6,\n     2,\n     2,\n     3,\n     2,\n     2,\n     2,\n     7,\n     5,\n     7,\n     0,\n     6,\n     3,\n     0,\n     2,\n     7,\n     7,\n     2,\n     6,\n     7,\n     7,\n     5,\n     6,\n     6,\n     6,\n     7,\n     7,\n     7,\n     6,\n     6,\n     7,\n     6,\n     2,\n     3,\n     7,\n     7,\n     6,\n     6,\n     7,\n     3,\n     2,\n     6,\n     3,\n     2,\n     7,\n     6,\n     5,\n     5,\n     2,\n     7,\n     6,\n     2,\n     2,\n     6,\n     3,\n     2,\n     7,\n     3,\n     5,\n     7,\n     5,\n     6,\n     7,\n     1,\n     6,\n     2,\n     3,\n     7,\n     2,\n     7,\n     2,\n     6,\n     6,\n     3,\n     6,\n     3,\n     7,\n     7,\n     5,\n     6,\n     6,\n     6,\n     6,\n     5,\n     2,\n     1,\n     6,\n     6,\n     2,\n     2,\n     2,\n     2,\n     6,\n     3,\n     7,\n     2,\n     6,\n     3,\n     6,\n     5,\n     2,\n     2,\n     1,\n     2,\n     7,\n     2,\n     0,\n     0,\n     6,\n     5,\n     7,\n     2,\n     2,\n     2,\n     6,\n     3,\n     1,\n     0,\n     3,\n     7,\n     3,\n     6,\n     6,\n     3,\n     7,\n     6,\n     5,\n     2,\n     7,\n     1,\n     7,\n     2,\n     6,\n     5,\n     6,\n     6,\n     0,\n     6,\n     6,\n     6,\n     1,\n     2,\n     7,\n     2,\n     6,\n     7,\n     1,\n     2,\n     6,\n     7,\n     2,\n     6,\n     7,\n     6,\n     2,\n     2,\n     6,\n     7,\n     6,\n     7,\n     2,\n     2,\n     2,\n     2,\n     6,\n     5,\n     2,\n     6,\n     7,\n     6,\n     7,\n     6,\n     2,\n     1,\n     1,\n     6,\n     6,\n     2,\n     6,\n     6,\n     6,\n     7,\n     6,\n     6,\n     7,\n     2,\n     7,\n     6,\n     6,\n     6,\n     7,\n     6,\n     2,\n     5,\n     5,\n     7,\n     6,\n     7,\n     6,\n     2,\n     6,\n     2,\n     7,\n     7,\n     1,\n     6,\n     6,\n     2,\n     6,\n     3,\n     2,\n     6,\n     6,\n     6,\n     7,\n     2,\n     7,\n     6,\n     1,\n     6,\n     6,\n     6,\n     6,\n     7,\n     2,\n     3,\n     3,\n     7,\n     1,\n     1,\n     2,\n     6,\n     6,\n     3,\n     2,\n     0,\n     7,\n     3,\n     6,\n     6,\n     5,\n     7,\n     7,\n     6,\n     0,\n     7,\n     0,\n     1,\n     3,\n     6,\n     6,\n     2,\n     6,\n     5,\n     6,\n     7,\n     6,\n     7,\n     7,\n     6,\n     5,\n     6,\n     6,\n     3,\n     2,\n     6,\n     6,\n     2,\n     6,\n     2,\n     2,\n     7,\n     6,\n     3,\n     6,\n     6,\n     5,\n     3,\n     2,\n     6,\n     6,\n     6,\n     5,\n     7,\n     4,\n     6,\n     6,\n     1,\n     2,\n     6,\n     6,\n     6,\n     6,\n     5,\n     2,\n     2,\n     2,\n     6,\n     2,\n     2,\n     6,\n     3,\n     0,\n     7,\n     6,\n     2,\n     5,\n     7,\n     5,\n     6,\n     6,\n     2,\n     5,\n     2,\n     7,\n     6,\n     3,\n     3,\n     5,\n     4,\n     5,\n     6,\n     6,\n     1,\n     2,\n     2,\n     3,\n     2,\n     6,\n     6,\n     6,\n     7,\n     6,\n     2,\n     6,\n     2,\n     5,\n     6,\n     7,\n     7,\n     6,\n     6,\n     3,\n     2,\n     7,\n     2,\n     6,\n     6,\n     2,\n     7,\n     5,\n     5,\n     3,\n     3,\n     7,\n     6,\n     6,\n     5,\n     2,\n     5,\n     7,\n     7,\n     7,\n     6,\n     6,\n     2,\n     7,\n     2,\n     6,\n     6,\n     6,\n     5,\n     6,\n     6,\n     6,\n     3,\n     6,\n     7,\n     6,\n     6,\n     3,\n     2,\n     2,\n     6,\n     6,\n     2,\n     6,\n     0,\n     6,\n     6,\n     3,\n     6,\n     6,\n     1,\n     7,\n     7,\n     6,\n     7,\n     6,\n     6,\n     6,\n     7,\n     6,\n     1,\n     6,\n     6,\n     6,\n     3,\n     6,\n     2,\n     6,\n     6,\n     6,\n     6,\n     2,\n     6,\n     3,\n     3,\n     6,\n     6,\n     2,\n     1,\n     0,\n     1,\n     2,\n     6,\n     5,\n     6,\n     7,\n     7,\n     6,\n     6,\n     6,\n     2,\n     ...]],\n   [[0,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     1,\n     7,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     1,\n     1,\n     1,\n     7,\n     0,\n     0,\n     7,\n     0,\n     7,\n     7,\n     6,\n     0,\n     0,\n     0,\n     7,\n     7,\n     0,\n     0,\n     0,\n     7,\n     1,\n     0,\n     0,\n     7,\n     1,\n     0,\n     0,\n     7,\n     7,\n     7,\n     7,\n     7,\n     0,\n     1,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     0,\n     0,\n     7,\n     7,\n     0,\n     7,\n     7,\n     0,\n     0,\n     0,\n     7,\n     7,\n     0,\n     7,\n     7,\n     5,\n     7,\n     1,\n     5,\n     7,\n     0,\n     0,\n     5,\n     7,\n     7,\n     7,\n     1,\n     0,\n     0,\n     0,\n     0,\n     7,\n     7,\n     7,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     7,\n     0,\n     7,\n     1,\n     7,\n     0,\n     6,\n     0,\n     1,\n     7,\n     0,\n     0,\n     7,\n     5,\n     0,\n     7,\n     1,\n     7,\n     7,\n     0,\n     7,\n     1,\n     7,\n     5,\n     0,\n     7,\n     0,\n     0,\n     7,\n     1,\n     7,\n     0,\n     7,\n     0,\n     1,\n     7,\n     0,\n     1,\n     7,\n     7,\n     7,\n     0,\n     0,\n     0,\n     7,\n     7,\n     5,\n     0,\n     0,\n     0,\n     0,\n     0,\n     7,\n     7,\n     2,\n     6,\n     0,\n     0,\n     0,\n     0,\n     7,\n     0,\n     7,\n     0,\n     7,\n     0,\n     0,\n     0,\n     7,\n     0,\n     0,\n     7,\n     7,\n     7,\n     0,\n     1,\n     0,\n     0,\n     7,\n     7,\n     7,\n     7,\n     7,\n     7,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     7,\n     0,\n     1,\n     0,\n     7,\n     7,\n     7,\n     1,\n     7,\n     0,\n     0,\n     7,\n     0,\n     7,\n     7,\n     7,\n     7,\n     1,\n     0,\n     1,\n     0,\n     0,\n     7,\n     0,\n     0,\n     7,\n     0,\n     0,\n     1,\n     0,\n     1,\n     0,\n     0,\n     0,\n     0,\n     0,\n     1,\n     0,\n     0,\n     5,\n     7,\n     0,\n     0,\n     0,\n     0,\n     7,\n     7,\n     0,\n     0,\n     0,\n     7,\n     6,\n     0,\n     7,\n     7,\n     1,\n     7,\n     7,\n     7,\n     0,\n     7,\n     1,\n     7,\n     0,\n     7,\n     7,\n     0,\n     7,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     0,\n     7,\n     0,\n     6,\n     7,\n     1,\n     5,\n     7,\n     7,\n     7,\n     0,\n     7,\n     7,\n     7,\n     0,\n     0,\n     0,\n     0,\n     1,\n     7,\n     7,\n     1,\n     0,\n     5,\n     0,\n     7,\n     0,\n     7,\n     7,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     7,\n     7,\n     0,\n     0,\n     0,\n     7,\n     0,\n     7,\n     7,\n     0,\n     7,\n     0,\n     0,\n     7,\n     7,\n     7,\n     0,\n     0,\n     0,\n     0,\n     7,\n     0,\n     0,\n     7,\n     0,\n     0,\n     7,\n     0,\n     7,\n     6,\n     5,\n     0,\n     0,\n     0,\n     7,\n     7,\n     7,\n     7,\n     7,\n     7,\n     7,\n     1,\n     1,\n     7,\n     0,\n     7,\n     1,\n     0,\n     7,\n     7,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     0,\n     7,\n     5,\n     7,\n     7,\n     7,\n     0,\n     7,\n     7,\n     7,\n     0,\n     7,\n     7,\n     1,\n     1,\n     7,\n     7,\n     5,\n     0,\n     0,\n     0,\n     0,\n     5,\n     7,\n     6,\n     0,\n     7,\n     7,\n     0,\n     0,\n     0,\n     7,\n     7,\n     0,\n     1,\n     1,\n     7,\n     0,\n     0,\n     7,\n     7,\n     5,\n     0,\n     0,\n     1,\n     5,\n     0,\n     5,\n     0,\n     7,\n     7,\n     0,\n     7,\n     0,\n     1,\n     0,\n     0,\n     0,\n     0,\n     7,\n     0,\n     1,\n     7,\n     1,\n     7,\n     0,\n     0,\n     7,\n     0,\n     7,\n     0,\n     7,\n     7,\n     7,\n     7,\n     1,\n     7,\n     0,\n     0,\n     1,\n     0,\n     1,\n     7,\n     1,\n     7,\n     7,\n     0,\n     0,\n     0,\n     7,\n     7,\n     7,\n     7,\n     7,\n     7,\n     7,\n     0,\n     0,\n     0,\n     0,\n     0,\n     0,\n     0,\n     7,\n     7,\n     5,\n     7,\n     7,\n     0,\n     7,\n     0,\n     7,\n     0,\n     7,\n     7,\n     0,\n     7,\n     7,\n     5,\n     7,\n     0,\n     7,\n     0,\n     7,\n     0,\n     5,\n     7,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     1,\n     7,\n     0,\n     6,\n     5,\n     7,\n     7,\n     0,\n     7,\n     7,\n     7,\n     0,\n     0,\n     0,\n     7,\n     7,\n     0,\n     7,\n     7,\n     7,\n     7,\n     5,\n     0,\n     7,\n     7,\n     0,\n     0,\n     7,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     7,\n     5,\n     1,\n     0,\n     0,\n     7,\n     7,\n     0,\n     1,\n     7,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     7,\n     7,\n     7,\n     0,\n     0,\n     0,\n     7,\n     7,\n     0,\n     0,\n     0,\n     1,\n     7,\n     1,\n     0,\n     0,\n     0,\n     7,\n     0,\n     0,\n     7,\n     7,\n     0,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     7,\n     7,\n     7,\n     5,\n     6,\n     7,\n     5,\n     0,\n     7,\n     0,\n     0,\n     7,\n     7,\n     7,\n     1,\n     7,\n     7,\n     7,\n     0,\n     7,\n     7,\n     0,\n     0,\n     7,\n     0,\n     0,\n     7,\n     7,\n     7,\n     0,\n     7,\n     7,\n     7,\n     0,\n     0,\n     7,\n     0,\n     7,\n     0,\n     7,\n     7,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     7,\n     0,\n     7,\n     7,\n     1,\n     7,\n     1,\n     0,\n     3,\n     1,\n     0,\n     0,\n     7,\n     7,\n     0,\n     7,\n     0,\n     0,\n     0,\n     7,\n     0,\n     7,\n     7,\n     7,\n     1,\n     0,\n     0,\n     7,\n     7,\n     1,\n     0,\n     1,\n     7,\n     0,\n     0,\n     0,\n     0,\n     0,\n     7,\n     7,\n     1,\n     0,\n     6,\n     7,\n     7,\n     1,\n     0,\n     0,\n     1,\n     0,\n     0,\n     0,\n     7,\n     7,\n     0,\n     7,\n     7,\n     0,\n     0,\n     0,\n     0,\n     7,\n     1,\n     5,\n     7,\n     7,\n     7,\n     0,\n     0,\n     7,\n     7,\n     0,\n     5,\n     0,\n     7,\n     1,\n     1,\n     0,\n     0,\n     1,\n     7,\n     0,\n     5,\n     0,\n     0,\n     7,\n     1,\n     0,\n     7,\n     0,\n     0,\n     7,\n     1,\n     0,\n     1,\n     7,\n     0,\n     6,\n     7,\n     0,\n     0,\n     0,\n     0,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     0,\n     1,\n     0,\n     0,\n     7,\n     0,\n     7,\n     0,\n     0,\n     1,\n     1,\n     0,\n     0,\n     0,\n     0,\n     7,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     7,\n     0,\n     0,\n     7,\n     7,\n     0,\n     0,\n     7,\n     1,\n     7,\n     6,\n     7,\n     0,\n     0,\n     0,\n     0,\n     7,\n     7,\n     1,\n     0,\n     0,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     7,\n     0,\n     7,\n     7,\n     1,\n     0,\n     0,\n     0,\n     0,\n     7,\n     0,\n     7,\n     7,\n     7,\n     1,\n     1,\n     0,\n     0,\n     0,\n     6,\n     0,\n     5,\n     7,\n     7,\n     0,\n     0,\n     7,\n     7,\n     7,\n     0,\n     5,\n     7,\n     7,\n     1,\n     7,\n     0,\n     7,\n     0,\n     0,\n     1,\n     7,\n     7,\n     0,\n     7,\n     7,\n     0,\n     1,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     0,\n     0,\n     0,\n     7,\n     7,\n     7,\n     0,\n     0,\n     7,\n     7,\n     0,\n     0,\n     0,\n     0,\n     1,\n     7,\n     1,\n     0,\n     7,\n     1,\n     0,\n     0,\n     0,\n     0,\n     0,\n     7,\n     7,\n     0,\n     0,\n     0,\n     0,\n     0,\n     7,\n     7,\n     5,\n     7,\n     6,\n     0,\n     7,\n     1,\n     7,\n     0,\n     0,\n     0,\n     5,\n     0,\n     7,\n     0,\n     7,\n     7,\n     7,\n     1,\n     1,\n     0,\n     6,\n     7,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     7,\n     0,\n     7,\n     7,\n     0,\n     0,\n     7,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     7,\n     1,\n     1,\n     7,\n     7,\n     7,\n     0,\n     0,\n     7,\n     0,\n     1,\n     7,\n     7,\n     7,\n     7,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     1,\n     0,\n     0,\n     0,\n     7,\n     0,\n     7,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     0,\n     7,\n     5,\n     0,\n     0,\n     7,\n     0,\n     0,\n     1,\n     7,\n     7,\n     5,\n     7,\n     7,\n     0,\n     0,\n     7,\n     0,\n     1,\n     0,\n     0,\n     7,\n     7,\n     0,\n     0,\n     0,\n     7,\n     0,\n     0,\n     0,\n     0,\n     7,\n     7,\n     7,\n     0,\n     0,\n     1,\n     5,\n     1,\n     0,\n     0,\n     1,\n     0,\n     7,\n     7,\n     0,\n     0,\n     7,\n     0,\n     ...]]]]]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2- Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "text = data_r8_processed['text']\n",
    "#df to a list\n",
    "rows = text.apply(''.join).tolist()\n",
    "# list of words for each document\n",
    "words = []\n",
    "for sublist in rows:\n",
    "    w = sublist.split()\n",
    "    words.append(w)\n",
    "# list that regroups all the words\n",
    "words_flat = []\n",
    "for sublist in words:\n",
    "    for item in sublist:\n",
    "        words_flat.append(item)\n",
    "#remove duplicates\n",
    "def remove_duplicates(l):\n",
    "    return list(set(l))\n",
    "vocabulary = remove_duplicates(words_flat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'open' from 'smart_open' (/home/wemblee/anaconda3/lib/python3.7/site-packages/smart_open/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-bc3d0782e9a9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mgensim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogging\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;31m# train word2vec on the two sentences\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgensim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mWord2Vec\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmin_count\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m#model_pretrained = models.KeyedVectors.load_word2vec_format('models_word2vec/GoogleNews-vectors-negative300.bin.gz', binary=True,limit = 1000000)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \"\"\"\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mgensim\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mparsing\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcorpora\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmatutils\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minterfaces\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msimilarities\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msummarization\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mutils\u001B[0m  \u001B[0;31m# noqa:F401\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mlogging\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/parsing/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mporter\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mPorterStemmer\u001B[0m  \u001B[0;31m# noqa:F401\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401\n\u001B[0m\u001B[1;32m      5\u001B[0m                             \u001B[0mstrip_tags\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstrip_short\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstrip_numeric\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m                             \u001B[0mstrip_non_alphanum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstrip_multiple_whitespaces\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/parsing/preprocessing.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mglob\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mgensim\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mgensim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparsing\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mporter\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mPorterStemmer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msix\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmoves\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 45\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0msmart_open\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mmultiprocessing\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcpu_count\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'open' from 'smart_open' (/home/wemblee/anaconda3/lib/python3.7/site-packages/smart_open/__init__.py)"
     ]
    }
   ],
   "source": [
    "import gensim, logging\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(words, min_count=1)\n",
    "#model_pretrained = models.KeyedVectors.load_word2vec_format('models_word2vec/GoogleNews-vectors-negative300.bin.gz', binary=True,limit = 1000000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accs_results = []\n",
    "fp_lists = []\n",
    "fn_lists = []\n",
    "for dataset_col in columns_labels_coclustering:\n",
    "    for method_colustering_col in dataset_col:\n",
    "        accuracy, fp_list, fn_list = execute_column_evaluations(method_colustering_col, model, vocabulary)\n",
    "        accs_results += [[accuracy]]\n",
    "        fp_lists += [[fp_list]]\n",
    "        fn_lists += [[fn_lists]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accs_results_df = pd.DataFrame(accs_results)\n",
    "accs_results_df.to_csv(\"results/accs_results_df.csv\", sep=',', index=False)\n",
    "fp_lists_df = pd.DataFrame(fp_lists)\n",
    "fp_lists_df.to_csv(\"results/fp_lists_df.csv\", sep=',', index=False)\n",
    "fn_lists_df = pd.DataFrame(fn_lists)\n",
    "fn_lists_df.to_csv(\"results/fn_lists_df.csv\", sep=',', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Algorithmes:\n",
    "### 1) pipelines pour l'importation des jeux de donnees\n",
    "### 2) algorithmes pour automatiser les resultats du coclustering est les mettres dans un tableau avec comme attribut [ \"method\", \"nmi\", \"ari\", \"acc\", \"dataset\"]\n",
    "### 3) Algorithmes pour calculer l'Accuracy et les L plus mauvaise FP et FN pour les clusters des colonnes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Resultats\n",
    "Les résultats ont tendance à diminuer systématiquement à mesure que α diminue.\n",
    "Ici, lorsque nous évaluons le clustering des colonnes, c'est CoclustInfo qui a les meilleur résultats, avec CoclustMod étant le moins perferoment algorithme et CoclustSpecMod entre les deux.\n",
    "Il est intéressant de noter que CoclustSpecMod n'a pas très bien fonctionné sur le clustering des lignes par rapport aux deux autres alors qu'ils fonctionnent relativement bien sur le clustering des colonnes.\n",
    " Nous établissons un score pour le regroupement des lignes, étant la moyenne de la trois métriques utilisées: NMI, ARI et précision de clustering, en effet, chacune de ces métriques a tendance à capturer\n",
    "différentes informations sur les performances et nous voulons un algorithme maximisant toutes. Pour le score du regroupement des colonnes, nous obtenons simplement la valeur la plus élevée pour chaque ensemble de données et algorithme, on choisit des seuil α = [0.85, 0.70, 0.45] ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "             method                  nmi                   ari  \\\n0        CoclustMod   0.3840246950328917    0.3190421774544417   \n1       CoclustInfo   0.4268043994602731    0.2749888124077073   \n2   CoclustModFuzzy  0.47719071468224405   0.34927249797899385   \n3        CoclustMod   0.9298244325626792    0.9603087444633237   \n4       CoclustInfo    0.933658618374583    0.9622041089489578   \n5   CoclustModFuzzy   0.8782489938126017    0.9036204373573509   \n6        CoclustMod   0.6608219684419236    0.6303911975823622   \n7       CoclustInfo   0.7240246081926431    0.6985729047551003   \n8   CoclustModFuzzy   0.0885949500478284  -0.04641735120127223   \n9        CoclustMod  0.38034475998690453    0.3341498869174912   \n10      CoclustInfo    0.464322338798633    0.2784553946973605   \n11  CoclustModFuzzy  0.47719071468224405   0.34927249797899385   \n\n                    acc   dataset  \n0    0.5035183737294762        r8  \n1   0.45725827469377117        r8  \n2    0.6215793588741204        r8  \n3    0.9866358262657414  classic3  \n4    0.9874068362888717  classic3  \n5    0.9655615523001799  classic3  \n6    0.8603241719520789  classic4  \n7    0.8887949260042284  classic4  \n8    0.3692741367159972  classic4  \n9    0.5410476935105551        r8  \n10  0.38141777430284074        r8  \n11   0.6215793588741204        r8  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>method</th>\n      <th>nmi</th>\n      <th>ari</th>\n      <th>acc</th>\n      <th>dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CoclustMod</td>\n      <td>0.3840246950328917</td>\n      <td>0.3190421774544417</td>\n      <td>0.5035183737294762</td>\n      <td>r8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CoclustInfo</td>\n      <td>0.4268043994602731</td>\n      <td>0.2749888124077073</td>\n      <td>0.45725827469377117</td>\n      <td>r8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CoclustModFuzzy</td>\n      <td>0.47719071468224405</td>\n      <td>0.34927249797899385</td>\n      <td>0.6215793588741204</td>\n      <td>r8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CoclustMod</td>\n      <td>0.9298244325626792</td>\n      <td>0.9603087444633237</td>\n      <td>0.9866358262657414</td>\n      <td>classic3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CoclustInfo</td>\n      <td>0.933658618374583</td>\n      <td>0.9622041089489578</td>\n      <td>0.9874068362888717</td>\n      <td>classic3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>CoclustModFuzzy</td>\n      <td>0.8782489938126017</td>\n      <td>0.9036204373573509</td>\n      <td>0.9655615523001799</td>\n      <td>classic3</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>CoclustMod</td>\n      <td>0.6608219684419236</td>\n      <td>0.6303911975823622</td>\n      <td>0.8603241719520789</td>\n      <td>classic4</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>CoclustInfo</td>\n      <td>0.7240246081926431</td>\n      <td>0.6985729047551003</td>\n      <td>0.8887949260042284</td>\n      <td>classic4</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>CoclustModFuzzy</td>\n      <td>0.0885949500478284</td>\n      <td>-0.04641735120127223</td>\n      <td>0.3692741367159972</td>\n      <td>classic4</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>CoclustMod</td>\n      <td>0.38034475998690453</td>\n      <td>0.3341498869174912</td>\n      <td>0.5410476935105551</td>\n      <td>r8</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>CoclustInfo</td>\n      <td>0.464322338798633</td>\n      <td>0.2784553946973605</td>\n      <td>0.38141777430284074</td>\n      <td>r8</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>CoclustModFuzzy</td>\n      <td>0.47719071468224405</td>\n      <td>0.34927249797899385</td>\n      <td>0.6215793588741204</td>\n      <td>r8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "Ce projet nous a permit de découvrir une méthode intéressante pour évaluer le clustering de colonnes d'un co-clustering.Cette étape est importante pour évaluer les algorithmes de co-clustering.\n",
    "Nous avons vu que le score de clustering des lignes et le score de clustering des colonnes ne sont pas nécessairement corrélés,\n",
    "et que pour certains ensembles de données, un algorithme peut donner de bons résultats pour un score et non pour l'autre.\n",
    "On peut suivre cette approache et  implémenter une méthode qui se base à trouver une similarité entre les documents (chaques documents est la somme des vecteurs (des mots)) deux à deux et la comparer avec la NMI, ARI..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}